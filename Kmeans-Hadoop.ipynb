{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63dd4a7c-f177-4f47-9406-b110e1e4e657",
   "metadata": {},
   "source": [
    "## Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e219e-794c-466b-97d2-247b1f518ff9",
   "metadata": {},
   "source": [
    "### Generated data: 2,000,000 rows and 5 columns.\n",
    "#### Name: kmeans_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007c6180-4ca8-40ce-af81-a169e9603dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "\n",
    "random.seed(1997)\n",
    "\n",
    "#Generate 2M 5-dimensional data points with random values between 0 and 1.\n",
    "#Change to '2000000' when run on the instance.\n",
    "k_meansData = [[random.random() for column in range(5)] for row in range(2000000)]\n",
    "\n",
    "with open('kmeans_data.csv', 'w') as f:\n",
    "    for row in k_meansData:\n",
    "        f.write(','.join(map(str, row)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38beaa-3e1b-4a3b-b3dd-83517cedf0a6",
   "metadata": {},
   "source": [
    "-- Code --\n",
    "\n",
    "vim generateKmeans_data.py\n",
    "\n",
    "python generateKmeans_data.py\n",
    "\n",
    "cat kmeans_data.csv | wc -l\n",
    "\n",
    "cat kmeans_data.csv | less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c9259-9aec-4a85-b972-69ea8081ffa4",
   "metadata": {},
   "source": [
    "### Generated initial cluster centers: 5 rows.\n",
    "#### Name: centers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da0a0b-3ea5-4d97-ba6f-6a18db88687a",
   "metadata": {},
   "source": [
    "#### Using Kmeans++ to reduce the potential of picking two close starting centroid. Try to run but took way to long to complete. --DONT run this code--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f25cb9-c81f-45c9-bc82-cac52c1294a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "import csv\n",
    "\n",
    "def squared_euclideanDistance(point1, point2):\n",
    "    '''Calculate square of Euclidean distance between 2 points.'''\n",
    "    return sum((x - y)**2 for x, y in zip(point1, point2))\n",
    "\n",
    "def kmeansCenters(data, kNum):\n",
    "    '''Get the initial cluster centers using Kmeans++.'''\n",
    "    \n",
    "    #Randomly choose the first center, stored as a list.\n",
    "    centers = [random.choice(data)]\n",
    "\n",
    "    for k in range(1, kNum):\n",
    "        \n",
    "        #Calculate squared distances to the nearest existing center for each point\n",
    "        distances = [min(squared_euclideanDistance(point, center) for center in centers) for point in data]\n",
    "\n",
    "        #Calculate probabilities\n",
    "        probabilities = [d / sum(distances) for d in distances]\n",
    "\n",
    "        #Choose the next center with probability proportional to squared distance.\n",
    "        nextCenter = random.choices(data, weights=probabilities)[0]\n",
    "        centers.append(nextCenter)\n",
    "\n",
    "    return centers\n",
    "\n",
    "#Read data from 'kmeans_data.csv'\n",
    "with open('kmeans_data.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    #Extract individual data from each cell.\n",
    "    data = [[float(value) for value in row] for row in reader]\n",
    "\n",
    "#Chosen number of k clusters.\n",
    "kNum = 5\n",
    "initialCenters = kmeansCenters(data, kNum)\n",
    "with open('centers.txt', 'w') as f:\n",
    "    for center in initialCenters:\n",
    "        f.write(','.join(map(str, center)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01fff5-01b6-4bd9-a0aa-9165885010b9",
   "metadata": {},
   "source": [
    "#### Using random selection of data points. --RUN this one--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def67bb4-9e5b-49e8-8c80-8ec0b2b45e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "import csv\n",
    "\n",
    "def clusterCenters(data, k):\n",
    "    '''Get the initial cluster centers using random points selection.'''\n",
    "    random.seed(1997)\n",
    "    return random.sample(data, k)\n",
    "\n",
    "with open('kmeans_data.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    #Extract individual data from each cell.\n",
    "    data = [[float(value) for value in row] for row in reader]\n",
    "\n",
    "#Chosen number of k clusters.\n",
    "kNum = 5\n",
    "initialCenters = clusterCenters(data, kNum)\n",
    "with open('centers.txt', 'w') as f:\n",
    "    for center in initialCenters:\n",
    "        f.write(','.join(map(str, center)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810af49-7312-4275-b893-c5c6ad3ef2ad",
   "metadata": {},
   "source": [
    "-- Code --\n",
    "\n",
    "vim generateCenters.py\n",
    "\n",
    "python vim generateCenters.py\n",
    "\n",
    "cat centers.txt | wc -l\n",
    "\n",
    "cat centers.txt | less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269971f-4696-4ecd-bab6-145caa748f76",
   "metadata": {},
   "source": [
    "### Kmeans Mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4d0fb2-1edf-48d6-a065-172a35916840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Detect whichever centers file being used for current iteration.\n",
    "input_centerFiles = ['centers.txt', 'centers1.txt', 'centers2.txt', 'centers3.txt', 'centers4.txt']\n",
    "\n",
    "#Try to open the file, if not work then pass.\n",
    "for input_centerFile in input_centerFiles:\n",
    "    try:\n",
    "        with open(input_centerFile, 'r') as centerFile:\n",
    "            centersDict = {}\n",
    "            for i, line in enumerate(centerFile):\n",
    "                centersDict[i]=[float(center) for center in line.strip().split(',')]\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "for dataPoint in sys.stdin:\n",
    "    #Get the five features, stored as a list.\n",
    "    featuresList = [float(feature) for feature in dataPoint.strip().split(',')]\n",
    "\n",
    "    #Calculate Euclidean distance between current data point and each centroid.\n",
    "    euclidDistance_list = [sum((c - f) ** 2 for c, f in zip(centroid, featuresList))\n",
    "                            for centroid in centersDict.values()]\n",
    "\n",
    "    #Get the cluster number associated with smallest distance.\n",
    "    clusterID = euclidDistance_list.index(min(euclidDistance_list))\n",
    "    #Output format: 'clusterNumber|feature1|feature2|feature3|feature4|feature5'\n",
    "    print(f\"{clusterID + 1}|{'|'.join(map(str, featuresList))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1f1a5-4ceb-4910-ac55-b90caec23ead",
   "metadata": {},
   "source": [
    "### Kmeans Reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e608401f-8e49-4289-88fa-641f1e9853f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys \n",
    "\n",
    "centersDict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    columns = line.strip().split('|')\n",
    "    clusterID = columns[0]\n",
    "    features = [float(value) for value in columns[1:]]\n",
    "    if clusterID not in centersDict:\n",
    "        centersDict[clusterID] = []\n",
    "\n",
    "    centersDict[clusterID].append(tuple(features))\n",
    "\n",
    "for clusterID, values in sorted(centersDict.items(), key=lambda x: int(x[0])):\n",
    "    dpSums = [sum(feature) for feature in zip(*values)]\n",
    "    dpCounts = [len(values)] * len(dpSums)\n",
    "    newCentroid = [dpSum / dpCount for dpSum, dpCount in zip(dpSums, dpCounts)]\n",
    "    print(','.join(map(str, newCentroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ac5f8-178c-4a6b-8612-c01e3251e07d",
   "metadata": {},
   "source": [
    "## Part 1.2 - Using the same data as 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46101205-487b-4ff7-8edf-c223f8c6753d",
   "metadata": {},
   "source": [
    "### Kmedian Mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c542fb08-5e34-421c-9508-9d311000522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Detect whichever centers file being used for current iteration.\n",
    "input_centerFiles = ['centers.txt', 'centers1.txt', 'centers2.txt', 'centers3.txt', 'centers4.txt']\n",
    "\n",
    "#Try to open the file, if not work then pass.\n",
    "for input_centerFile in input_centerFiles:\n",
    "    try:\n",
    "        with open(input_centerFile, 'r') as centerFile:\n",
    "            centersDict = {}\n",
    "            for i, line in enumerate(centerFile):\n",
    "                centersDict[i]=[float(center) for center in line.strip().split(',')]\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "for dataPoint in sys.stdin:\n",
    "    #Get the five features, stored as a list.\n",
    "    featuresList = [float(feature) for feature in dataPoint.strip().split(',')]\n",
    "\n",
    "    #Calculate Manhattan distance between current data point and each centroid.\n",
    "    manhattanDistance_list = [sum(abs(c - f) for c, f in zip(centroid, featuresList))\n",
    "                               for centroid in centersDict.values()]\n",
    "\n",
    "    #Get the cluster number associated with smallest distance.\n",
    "    clusterID = manhattanDistance_list.index(min(manhattanDistance_list))\n",
    "    #Output format: 'clusterNumber|feature1|feature2|feature3|feature4|feature5'\n",
    "    print(f\"{clusterID + 1}|{'|'.join(map(str, featuresList))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2625b07-3b2b-46f1-8a1a-daa5b4e23da8",
   "metadata": {},
   "source": [
    "### Kmedian Reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0793ac2f-70ba-4c6f-8cb0-17c2fdd477d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys \n",
    "\n",
    "centersDict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    columns = line.strip().split('|')\n",
    "    clusterID = columns[0]\n",
    "    features = [float(value) for value in columns[1:]]\n",
    "    if clusterID not in centersDict:\n",
    "        centersDict[clusterID] = []\n",
    "\n",
    "    centersDict[clusterID].append(tuple(features))\n",
    "\n",
    "#Iterate through each cluster.\n",
    "#Each data point (5 features) stored as as an element in centersDict value list.\n",
    "for clusterID, values in sorted(centersDict.items(), key=lambda x: int(x[0])):\n",
    "    newCentroid = []\n",
    "    \n",
    "    #Iterate through each feature.\n",
    "    for featureIndex in range(len(values[0])): \n",
    "        \n",
    "        #Extract each feature from all data points within a clusters. \n",
    "        #Put into a list and sort it.\n",
    "        featureValues = sorted([value[featureIndex] for value in values])\n",
    "        \n",
    "        #Return median index as integer. I.e,: If 3.5 return 3.\n",
    "        medianIndex = len(featureValues) // 2\n",
    "        \n",
    "        #If total number of data point assigned to a cluster is even,\n",
    "        #Get average value of two middle points.\n",
    "        if len(featureValues) % 2 == 0:\n",
    "            median = (featureValues[medianIndex - 1] + featureValues[medianIndex]) / 2\n",
    "        \n",
    "        #Else, get middle value.\n",
    "        else:\n",
    "            median = featureValues[medianIndex]\n",
    "\n",
    "        newCentroid.append(median)\n",
    "\n",
    "    print(','.join(map(str, newCentroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e245b-5d87-4e70-8c29-4c05428a6e8d",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bb7ece4-b6a3-40ae-9925-1ff1e5ba1c47",
   "metadata": {},
   "source": [
    "select sum(lo_revenue), count(*), d_year, p_category\n",
    "from lineorder, dwdate, part \n",
    "where lo_orderdate = d_datekey and lo_partkey = p_partkey\n",
    "and d_sellingseason = 'Fall' and p_brand1 = 'MFGR#2123' group by d_year, p_category;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7b9e4-bcf8-42fe-9b44-e3c29bb8b778",
   "metadata": {},
   "source": [
    "### 1st MapReduce\n",
    "#### Mapper 1 - Extract column name using index and filtered by indicated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bc1b9-2dc1-4a43-afa6-653ccdbef3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    columnName = line.strip().split('|')\n",
    "    \n",
    "    #Extract relevant columns in lineorder table.\n",
    "    if columnName[1].isdigit():\n",
    "        lo_revenue = columnName[12]\n",
    "        lo_orderdate = columnName[5]\n",
    "        lo_partkey = columnName[3]\n",
    "        print(f\"lineorder|{lo_revenue}|{lo_orderdate}|{lo_partkey}\") \n",
    "    \n",
    "    #Extract relevant columns in part table.\n",
    "    #Tackle condition p_brand1 = 'MFGR#2123'.\n",
    "    else:\n",
    "        p_category = columnName[3]\n",
    "        p_partkey = columnName[0]\n",
    "        p_brand1 = columnName[4]\n",
    "        if p_brand1 == 'MFGR#2123': \n",
    "            print(f\"part|{p_category}|{p_partkey}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0ab6b-e50c-467f-963b-5cc34f8aa565",
   "metadata": {},
   "source": [
    "#### Reducer 1 - Join part and lineorder table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9447b664-93c2-4e62-a469-29f1a10cda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Dictionaries to store data from each table.\n",
    "#Using join condition as key.\n",
    "partDict = {}\n",
    "lineorderDict = {}\n",
    "#List to store join data.\n",
    "resData = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    columnName = line.strip().split('|')\n",
    "    fileSource = columnName[0]\n",
    "    \n",
    "    #Using p_partkey as key for part table.\n",
    "    if fileSource == \"part\":\n",
    "        p_category = columnName[1]\n",
    "        p_partkey = columnName[2]\n",
    "        \n",
    "        if p_partkey not in partDict:\n",
    "            partDict[p_partkey] = []\n",
    "        partDict[p_partkey].append(p_category)\n",
    "\n",
    "    #Using lo_partkey as key for lineorder table.\n",
    "    elif fileSource == \"lineorder\":\n",
    "        lo_revenue = columnName[1]\n",
    "        lo_orderdate = columnName[2]\n",
    "        lo_partkey = columnName[3]\n",
    "        \n",
    "        if lo_partkey not in lineorderDict:\n",
    "            lineorderDict[lo_partkey] = []\n",
    "        lineorderDict[lo_partkey].append((lo_revenue, lo_orderdate))\n",
    "\n",
    "#Tackle lo_orderdate = d_datekey.\n",
    "for lineorderKey, lineorderValues in lineorderDict.items():\n",
    "    if lineorderKey in partDict:\n",
    "        p_categoryList = partDict[lineorderKey]\n",
    "        for p_category in p_categoryList:\n",
    "            #Tackle multiple rows with the same lo_partkey.\n",
    "            for lineorderValue in lineorderValues: \n",
    "                resData.append((p_category, lineorderValue[0], lineorderValue[1]))\n",
    "\n",
    "#Join data are stored in resData list. Given each row is each element.\n",
    "for variable in resData:\n",
    "    p_category = variable[0]\n",
    "    lo_revenue = variable[1]\n",
    "    lo_orderdate = variable[2]\n",
    "    print(f\"{p_category}|{lo_revenue}|{lo_orderdate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7713053-7408-4099-ad05-45543bcbae1a",
   "metadata": {},
   "source": [
    "### 2nd MapReduce\n",
    "#### Mapper 2 - Extract column name using index and filtered by indicated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a82b5be-683e-43b5-af78-ee3c7e4d6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#Dictionary to store data for dwdate table.\n",
    "dwdateDict = {}\n",
    "\n",
    "with open('dwdate.tbl', 'r') as cacheFile:\n",
    "    for line in cacheFile:\n",
    "        columnName = line.strip().split('|')\n",
    "        d_datekey = columnName[0]\n",
    "        d_year = columnName[4]\n",
    "        d_sellingseason = columnName[12]\n",
    "\n",
    "        #Tackle d_sellingseason = 'Fall'.\n",
    "        if d_sellingseason == 'Fall':\n",
    "            #Incase there are multiple d_year associated with one d_datekey.\n",
    "            if d_datekey not in dwdateDict:\n",
    "                dwdateDict[d_datekey] = []\n",
    "            dwdateDict[d_datekey].append(d_year)\n",
    "\n",
    "#Tackle output from 1st MapReduce.\n",
    "for line in sys.stdin:\n",
    "    columnName = line.strip().split('|')\n",
    "    p_category = columnName[0]\n",
    "    lo_revenue = columnName[1]\n",
    "    lo_orderdate = columnName[2]\n",
    "    \n",
    "    #Tackle lo_orderdate = d_datekey.\n",
    "    if lo_orderdate in dwdateDict:\n",
    "        d_yearList = dwdateDict[lo_orderdate]\n",
    "        for d_year in d_yearList:\n",
    "            print(f\"{d_year}|{p_category}|{lo_revenue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216819b3-377f-4c78-b0e0-23b38db30771",
   "metadata": {},
   "source": [
    "#### Reducer 2 - Perform groupby and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5bbfaf1-b1e3-4ced-9031-c89d9580e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(lo_revenue)|count(*)|d_year|p_category\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Initiate counters to calculate sum and respective total count.\n",
    "revSum = 0\n",
    "revCount = 0\n",
    "#Dictionary to store aggregated values.\n",
    "joinDict = {}\n",
    "\n",
    "print('sum(lo_revenue)|count(*)|d_year|p_category')\n",
    "\n",
    "for line in sys.stdin:\n",
    "    columnName = line.strip().split('|')\n",
    "    d_year = columnName[0]\n",
    "    p_category = columnName[1]\n",
    "    lo_revenue = int(columnName[2])\n",
    "    \n",
    "    #Tackle group by d_year, p_category.\n",
    "    #Stored as tuple for dictionary key.\n",
    "    if (d_year, p_category) not in joinDict: #If new lo_orderdate.\n",
    "        joinDict[(d_year, p_category)] = {'revSum': 0, 'revCount': 0}\n",
    "            \n",
    "    #Keep appending.\n",
    "    joinDict[(d_year, p_category)]['revSum'] += lo_revenue\n",
    "    joinDict[(d_year, p_category)]['revCount'] += 1\n",
    "\n",
    "#For each item, sum revenue output is already calculated here.\n",
    "for keys, values in joinDict.items():\n",
    "    print(f\"{values['revSum']}|{values['revCount']}|{keys[0]}|{keys[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38851e0f-f81f-426b-91a5-6c8f2d2c9ca4",
   "metadata": {},
   "source": [
    "### Command - Using Hadoop Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d235d-23a1-47b0-828d-b6632faa7906",
   "metadata": {},
   "source": [
    "--1st MR -- \\\n",
    "time hadoop jar hadoop-streaming-2.6.4.jar -input /data/lineorder.tbl,/data/part.tbl -output /data/Part2_MR1_Output -mapper part2_mapper1.py -reducer part2_reducer1.py -file part2_mapper1.py -file part2_reducer1.py\n",
    "\n",
    "--2nd MR -- \\\n",
    "time hadoop jar hadoop-streaming-2.6.4.jar -input /data/Part2_MR1_Output/part-00000 -output /data/Part2_MR2_Output -mapper part2_mapper2.py -reducer part2_reducer2.py -file part2_mapper2.py -file part2_reducer2.py -file dwdate.tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5296389-66c5-4ee2-ba5c-40d4935e2424",
   "metadata": {},
   "source": [
    "### Command - Using MapReduceChain.java"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf592cbd-f7bf-472d-ade7-5396079df548",
   "metadata": {},
   "source": [
    "import org.apache.hadoop.util.Tool;\n",
    "import org.apache.hadoop.mapred.JobConf;\n",
    "import org.apache.hadoop.mapred.jobcontrol.Job;\n",
    "import org.apache.hadoop.mapred.jobcontrol.JobControl;\n",
    "import org.apache.hadoop.conf.Configured;\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.util.ToolRunner;\n",
    "import org.apache.hadoop.streaming.StreamJob;\n",
    "\n",
    "\n",
    "public class MapReduceChain extends Configured implements Tool\n",
    "{\n",
    "    public int run( String[] args) throws Exception\n",
    "    {\n",
    "        JobControl jobControl = new JobControl( \"Part2\");\n",
    "\n",
    "        String[] job1Args = new String[]\n",
    "        {\n",
    "            \"-mapper\"   , \"part2_mapper1.py\",\n",
    "            \"-reducer\"  , \"part2_reducer1.py\",\n",
    "            \"-input\"    , \"/data/lineorder.tbl\",\n",
    "            \"-input\"    , \"/data/part.tbl\",\n",
    "            \"-output\"   , \"/Part2_MR1_Output/\",\n",
    "            \"-file\"\t    , \"part2_mapper1.py\",\n",
    "            \"-file\"\t    , \"part2_reducer1.py\"\n",
    "        };\n",
    "        JobConf job1Conf = StreamJob.createJob(job1Args);\n",
    "        Job job1 = new Job(job1Conf);\n",
    "        jobControl.addJob(job1);\n",
    "\n",
    "        String[] job2Args = new String[]\n",
    "        {\n",
    "            \"-mapper\"   , \"part2_mapper2.py\",\n",
    "            \"-reducer\"  , \"part2_reducer2.py\",\n",
    "            \"-input\"    , \"/Part2_MR1_Output/part-00000\",\n",
    "            \"-output\"   , \"/Part2_MR2_Output/\",\n",
    "            \"-file\"     , \"part2_mapper2.py\",\n",
    "            \"-file\"     , \"part2_reducer2.py\",\n",
    "            \"-file\"     , \"dwdate.tbl\"\n",
    "        };\n",
    "        JobConf job2Conf = StreamJob.createJob(job2Args);\n",
    "        Job job2 = new Job(job2Conf);\n",
    "        job2.addDependingJob(job1);\n",
    "        jobControl.addJob(job2);\n",
    "\n",
    "        Thread runJobControl = new Thread(jobControl);\n",
    "        runJobControl.start();\n",
    "        while(!jobControl.allFinished())\n",
    "        {\n",
    "            // wait here\n",
    "        }\n",
    "\n",
    "        return 0;\n",
    "    }\n",
    "\n",
    "    public static void main( String[] args) throws Exception\n",
    "    {\n",
    "        int result = ToolRunner.run(new Configuration(), new MapReduceChain(), args);\n",
    "        System.exit(result);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9de1d-55e3-4266-903e-e4cb4c9e2b1e",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef3d14-b28c-40e4-bc4a-8b7c6b608705",
   "metadata": {},
   "source": [
    "vim MapReduceChain.java\n",
    "\n",
    "javac -cp ~/hadoop-streaming-2.6.4.jar:$(hadoop classpath) MapReduceChain.java\n",
    "\n",
    "jar -cf MapReduceChain.jar MapReduceChain.class\n",
    "\n",
    "jar xvf MapReduceChain.jar META-INF/MANIFEST.MF\n",
    "\n",
    "nano META-INF/MANIFEST.MF\n",
    "-- Paste in: Class-Path: /home/ec2-user/hadoop-streaming-2.6.4.jar --\n",
    "\n",
    "jar cmf META-INF/MANIFEST.MF MapReduceChain.jar MapReduceChain.class\n",
    "\n",
    "hadoop jar MapReduceChain.jar MapReduceChain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
